<!DOCTYPE html>
<!-- Generated by pkgdown: do not edit by hand --><html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<title>Like Micrograd, But Worse • femtograd</title>
<script src="deps/jquery-3.6.0/jquery-3.6.0.min.js"></script><meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<link href="deps/bootstrap-5.3.1/bootstrap.min.css" rel="stylesheet">
<script src="deps/bootstrap-5.3.1/bootstrap.bundle.min.js"></script><link href="deps/font-awesome-6.5.2/css/all.min.css" rel="stylesheet">
<link href="deps/font-awesome-6.5.2/css/v4-shims.min.css" rel="stylesheet">
<script src="deps/headroom-0.11.0/headroom.min.js"></script><script src="deps/headroom-0.11.0/jQuery.headroom.min.js"></script><script src="deps/bootstrap-toc-1.0.1/bootstrap-toc.min.js"></script><script src="deps/clipboard.js-2.0.11/clipboard.min.js"></script><script src="deps/search-1.0.0/autocomplete.jquery.min.js"></script><script src="deps/search-1.0.0/fuse.min.js"></script><script src="deps/search-1.0.0/mark.min.js"></script><!-- pkgdown --><script src="pkgdown.js"></script><meta property="og:title" content="Like Micrograd, But Worse">
<meta name="description" content="Let's you construct differentiable expressions using auto differentiation.">
<meta property="og:description" content="Let's you construct differentiable expressions using auto differentiation.">
</head>
<body>
    <a href="#main" class="visually-hidden-focusable">Skip to contents</a>


    <nav class="navbar navbar-expand-lg fixed-top bg-light" data-bs-theme="light" aria-label="Site navigation"><div class="container">

    <a class="navbar-brand me-2" href="index.html">femtograd</a>

    <small class="nav-text text-muted me-auto" data-bs-toggle="tooltip" data-bs-placement="bottom" title="">0.9000</small>


    <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbar" aria-controls="navbar" aria-expanded="false" aria-label="Toggle navigation">
      <span class="navbar-toggler-icon"></span>
    </button>

    <div id="navbar" class="collapse navbar-collapse ms-3">
      <ul class="navbar-nav me-auto">
<li class="nav-item"><a class="nav-link" href="reference/index.html">Reference</a></li>
      </ul>
<ul class="navbar-nav">
<li class="nav-item"><form class="form-inline" role="search">
 <input class="form-control" type="search" name="search-input" id="search-input" autocomplete="off" aria-label="Search site" placeholder="Search for" data-search-index="search.json">
</form></li>
      </ul>
</div>


  </div>
</nav><div class="container template-home">
<div class="row">
  <main id="main" class="col-md-9"><div class="section level1">
<div class="page-header"><h1 id="femtograd">femtograd<a class="anchor" aria-label="anchor" href="#femtograd"></a>
</h1></div>
<ul>
<li><a href="#exponential-distribution" id="toc-exponential-distribution">Exponential distribution</a></li>
<li><a href="#automatic-differentiation-ad" id="toc-automatic-differentiation-ad">Automatic differentiation (AD)</a></li>
</ul>
<p>Here’s a quick demonstration of how to use the <a href="https://github.com/queelius/femtograd" class="external-link"><code>femtograd</code></a> R package.</p>
<p>Load it like this:</p>
<div class="sourceCode" id="cb1"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="kw"><a href="https://rdrr.io/r/base/library.html" class="external-link">library</a></span><span class="op">(</span><span class="va">femtograd</span><span class="op">)</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; Attaching package: 'femtograd'</span></span>
<span><span class="co">#&gt; The following object is masked from 'package:utils':</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt;     data</span></span></code></pre></div>
<div class="section level2">
<h2 id="exponential-distribution">Exponential distribution<a class="anchor" aria-label="anchor" href="#exponential-distribution"></a>
</h2>
<p>Let’s create a simple loglikelihood function for the exponential distribution paramterized by <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>λ</mi><annotation encoding="application/x-tex">\lambda</annotation></semantics></math> (failure rate).</p>
<p>We have</p>
<p><math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>f</mi><msub><mi>T</mi><mi>i</mi></msub></msub><mrow><mo stretchy="true" form="prefix">(</mo><msub><mi>t</mi><mi>i</mi></msub><mo stretchy="false" form="prefix">|</mo><mi>λ</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo>=</mo><mi>λ</mi><mo>exp</mo><mrow><mo stretchy="true" form="prefix">(</mo><mo>−</mo><mi>λ</mi><msub><mi>t</mi><mi>i</mi></msub><mo stretchy="true" form="postfix">)</mo></mrow><mi>.</mi></mrow><annotation encoding="application/x-tex">
  f_{T_i}(t_i | \lambda) = \lambda \exp(-\lambda t_i).
</annotation></semantics></math></p>
<p>So, the loglikelihood function is just</p>
<p><math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo>ℓ</mo><mrow><mo stretchy="true" form="prefix">(</mo><mi>λ</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo>=</mo><mi>n</mi><mo>log</mo><mi>λ</mi><mo>−</mo><mi>λ</mi><munderover><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>n</mi></munderover><msub><mi>t</mi><mi>i</mi></msub><mi>.</mi></mrow><annotation encoding="application/x-tex">
  \ell(\lambda) = n \log \lambda - \lambda \sum_{i=1}^n t_i.
</annotation></semantics></math></p>
<p>Let’s generate <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>n</mi><mo>=</mo><mn>30</mn></mrow><annotation encoding="application/x-tex">n=30</annotation></semantics></math> observations.</p>
<div class="sourceCode" id="cb2"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">n</span> <span class="op">&lt;-</span> <span class="fl">30</span></span>
<span><span class="va">true_rate</span> <span class="op">&lt;-</span> <span class="fl">7.3</span></span>
<span><span class="va">data</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/Exponential.html" class="external-link">rexp</a></span><span class="op">(</span><span class="va">n</span>,<span class="va">true_rate</span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/utils/head.html" class="external-link">head</a></span><span class="op">(</span><span class="va">data</span><span class="op">)</span></span>
<span><span class="co">#&gt; [1] 0.102432677 0.009849689 0.037429092 0.093926112 0.033046038 0.181780460</span></span>
<span></span>
<span><span class="op">(</span><span class="va">mle.rate</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/MathFun.html" class="external-link">abs</a></span><span class="op">(</span><span class="fl">1</span><span class="op">/</span><span class="fu"><a href="https://rdrr.io/r/base/mean.html" class="external-link">mean</a></span><span class="op">(</span><span class="va">data</span><span class="op">)</span><span class="op">)</span><span class="op">)</span></span>
<span><span class="co">#&gt; [1] 6.245535</span></span></code></pre></div>
<p>We see that the MLE <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mover><mi>θ</mi><mo accent="true">̂</mo></mover><annotation encoding="application/x-tex">\hat\theta</annotation></semantics></math> is 6.2455349.</p>
</div>
</div>
<div class="section level1">
<h1 id="automatic-differentiation-ad">Automatic differentiation (AD)<a class="anchor" aria-label="anchor" href="#automatic-differentiation-ad"></a>
</h1>
<p>Finding the value (argmax) that maximizes the log-likelihood function is trivial to solve in this case, and it has a closed-form solution. However, to demonstrate the use of <code>femtograd</code>, we will construct a <code>loglike_exp</code> function generator that returns an object that can be automatically differentiated (AD) using backpropogation, which is an efficient way of applying the chain-rule to expressions (like <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo>exp</mo><mo stretchy="false" form="prefix">{</mo><mi>y</mi><mi>a</mi><msup><mi>x</mi><mn>2</mn></msup><mo stretchy="false" form="postfix">}</mo></mrow><annotation encoding="application/x-tex">\exp\{y a x^2\}</annotation></semantics></math> using a <em>computational graph</em> that represents the expression.</p>
<p>These kind of computational graphs have the nice property that for any differentiable expression that we can model in software, its partial derivative with respect to some node in the graph can be efficiently and accurately computed without resorting to numerical finite difference methods or slow, potentially difficult to compose symbolic methods.</p>
<p>There are many libraries that do this. This library itself is based on the excellent work by Karpathy who developed the Python library known as <a href="https://github.com/karpathy/micrograd" class="external-link"><code>micrograd</code></a>, which was developed for the explicit purpose of teaching the basic concept of AD and backpropagation for minimizing loss functions for neural networks.</p>
<p>Let’s solve for the MLE iteratively as a demonstration of how to use <a href="https://github.com/queelius/femtograd" class="external-link"><code>femtograd</code></a>. First, we construct the log-likelihood generator:</p>
<div class="sourceCode" id="cb3"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">loglike_exp</span> <span class="op">&lt;-</span> <span class="kw">function</span><span class="op">(</span><span class="va">rate</span>, <span class="va">data</span><span class="op">)</span></span>
<span><span class="op">{</span></span>
<span>  <span class="kw"><a href="https://rdrr.io/r/base/function.html" class="external-link">return</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/Log.html" class="external-link">log</a></span><span class="op">(</span><span class="va">rate</span><span class="op">)</span><span class="op">*</span><span class="fu"><a href="https://rdrr.io/r/base/length.html" class="external-link">length</a></span><span class="op">(</span><span class="va">data</span><span class="op">)</span> <span class="op">-</span> <span class="va">rate</span> <span class="op">*</span> <span class="fu"><a href="https://rdrr.io/r/base/sum.html" class="external-link">sum</a></span><span class="op">(</span><span class="va">data</span><span class="op">)</span><span class="op">)</span></span>
<span><span class="op">}</span></span></code></pre></div>
<p>Initially, we guess that <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mover><mi>λ</mi><mo accent="true">̂</mo></mover><annotation encoding="application/x-tex">\hat\lambda</annotation></semantics></math> is <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mn>1</mn><annotation encoding="application/x-tex">1</annotation></semantics></math>, which is a terrible estimate.</p>
<div class="sourceCode" id="cb4"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">rate</span> <span class="op">&lt;-</span> <span class="fu"><a href="reference/val.html">val</a></span><span class="op">(</span><span class="fl">1</span><span class="op">)</span></span></code></pre></div>
<p>Gradient clipping is a technique to prevent taking too large of a step when gradients become too large (remember that gradients are a <em>local</em> feature, so we generally should not use it to take too big of a step) during optimization, which can cause instability or overshooting the optimal value. By limiting the step size, gradient clipping helps ensure that the optimization takes smaller, more stable steps.</p>
<p>Here is the R code:</p>
<div class="sourceCode" id="cb5"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co"># Takes a gradient `g` and an optional `max_norm` parameter, which defaults</span></span>
<span><span class="co"># to 1. It calculates the gradient's L2 norm (Euclidean norm) and scales the</span></span>
<span><span class="co"># gradient down if its norm exceeds the specified max_norm. This is used during</span></span>
<span><span class="co"># the gradient ascent loop to help ensure stable optimization.</span></span>
<span><span class="va">grad_clip</span> <span class="op">&lt;-</span> <span class="kw">function</span><span class="op">(</span><span class="va">g</span>, <span class="va">max_norm</span> <span class="op">=</span> <span class="fl">1</span><span class="op">)</span> <span class="op">{</span></span>
<span>  <span class="va">norm</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/MathFun.html" class="external-link">sqrt</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/sum.html" class="external-link">sum</a></span><span class="op">(</span><span class="va">g</span> <span class="op">*</span> <span class="va">g</span><span class="op">)</span><span class="op">)</span></span>
<span>  <span class="kw">if</span> <span class="op">(</span><span class="va">norm</span> <span class="op">&gt;</span> <span class="va">max_norm</span><span class="op">)</span> <span class="op">{</span></span>
<span>    <span class="va">g</span> <span class="op">&lt;-</span> <span class="op">(</span><span class="va">max_norm</span> <span class="op">/</span> <span class="va">norm</span><span class="op">)</span> <span class="op">*</span> <span class="va">g</span></span>
<span>  <span class="op">}</span></span>
<span>  <span class="va">g</span></span>
<span><span class="op">}</span></span></code></pre></div>
<p>We find the MLE using a simple iteration (200 loops).</p>
<div class="sourceCode" id="cb6"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">loglik</span> <span class="op">&lt;-</span> <span class="fu">loglike_exp</span><span class="op">(</span><span class="va">rate</span>, <span class="va">data</span><span class="op">)</span></span>
<span><span class="va">lr</span> <span class="op">&lt;-</span> <span class="fl">0.2</span> <span class="co"># learning rate</span></span>
<span><span class="kw">for</span> <span class="op">(</span><span class="va">i</span> <span class="kw">in</span> <span class="fl">1</span><span class="op">:</span><span class="fl">200</span><span class="op">)</span></span>
<span><span class="op">{</span></span>
<span>  <span class="fu">zero_grad</span><span class="op">(</span><span class="va">loglik</span><span class="op">)</span></span>
<span>  <span class="fu"><a href="reference/backward.html">backward</a></span><span class="op">(</span><span class="va">loglik</span><span class="op">)</span></span>
<span></span>
<span>  <span class="fu"><a href="reference/data.html">data</a></span><span class="op">(</span><span class="va">rate</span><span class="op">)</span> <span class="op">&lt;-</span> <span class="fu"><a href="reference/data.html">data</a></span><span class="op">(</span><span class="va">rate</span><span class="op">)</span> <span class="op">+</span> <span class="va">lr</span> <span class="op">*</span> <span class="fu">grad_clip</span><span class="op">(</span><span class="fu"><a href="reference/grad.html">grad</a></span><span class="op">(</span><span class="va">rate</span><span class="op">)</span><span class="op">)</span></span>
<span>  <span class="kw">if</span> <span class="op">(</span><span class="va">i</span> <span class="op"><a href="https://rdrr.io/r/base/Arithmetic.html" class="external-link">%%</a></span> <span class="fl">50</span> <span class="op">==</span> <span class="fl">0</span><span class="op">)</span></span>
<span>    <span class="fu"><a href="https://rdrr.io/r/base/cat.html" class="external-link">cat</a></span><span class="op">(</span><span class="st">"iteration"</span>, <span class="va">i</span>, <span class="st">", rate ="</span>, <span class="fu"><a href="reference/data.html">data</a></span><span class="op">(</span><span class="va">rate</span><span class="op">)</span>, <span class="st">", drate/dl ="</span>, <span class="fu"><a href="reference/grad.html">grad</a></span><span class="op">(</span><span class="va">rate</span><span class="op">)</span>, <span class="st">"\n"</span><span class="op">)</span></span>
<span><span class="op">}</span></span>
<span><span class="co">#&gt; iteration 50 , rate = 6.238781 , drate/dl = 0.006148105 </span></span>
<span><span class="co">#&gt; iteration 100 , rate = 6.245533 , drate/dl = 1.447689e-06 </span></span>
<span><span class="co">#&gt; iteration 150 , rate = 6.245535 , drate/dl = 3.418377e-10 </span></span>
<span><span class="co">#&gt; iteration 200 , rate = 6.245535 , drate/dl = 8.082424e-14</span></span></code></pre></div>
<p>Did the gradient ascent method converge to the MLE?</p>
<div class="sourceCode" id="cb7"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="op">(</span><span class="va">converged</span> <span class="op">&lt;-</span> <span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/MathFun.html" class="external-link">abs</a></span><span class="op">(</span><span class="va">mle.rate</span> <span class="op">-</span> <span class="fu"><a href="reference/data.html">data</a></span><span class="op">(</span><span class="va">rate</span><span class="op">)</span><span class="op">)</span> <span class="op">&lt;</span> <span class="fl">1e-3</span><span class="op">)</span><span class="op">)</span></span>
<span><span class="co">#&gt; [1] TRUE</span></span></code></pre></div>
<p>It’s worth pointing out that we did not update <code>loglik</code> in the gradient ascent loop, since we only needed the gradient (score) in this case. If, however, we had needed to know the log-likelihood for some reason, such as when using a line search method to avoid overshooting, we would need to update with <code>loglik &lt;- loglike_exp(rate, data)</code> each time through the loop.</p>
</div>

  </main><aside class="col-md-3"><div class="license">
<h2 data-toc-skip>License</h2>
<ul class="list-unstyled">
<li><a href="LICENSE.html">Full license</a></li>
<li><small>GPL (&gt;= 3)</small></li>
</ul>
</div>


<div class="citation">
<h2 data-toc-skip>Citation</h2>
<ul class="list-unstyled">
<li><a href="authors.html#citation">Citing femtograd</a></li>
</ul>
</div>

<div class="developers">
<h2 data-toc-skip>Developers</h2>
<ul class="list-unstyled">
<li>Alexander Towell <br><small class="roles"> Author, maintainer </small> <a href="https://orcid.org/0000-0001-6443-9897" target="orcid.widget" aria-label="ORCID" class="external-link"><span class="fab fa-orcid orcid" aria-hidden="true"></span></a>  </li>
</ul>
</div>



  </aside>
</div>


    <footer><div class="pkgdown-footer-left">
  <p>Developed by Alexander Towell.</p>
</div>

<div class="pkgdown-footer-right">
  <p>Site built with <a href="https://pkgdown.r-lib.org/" class="external-link">pkgdown</a> 2.2.0.</p>
</div>

    </footer>
</div>





  </body>
</html>
