---
title: "femtograd: Introduction"
output:
  rmarkdown::html_vignette:
    toc: true
vignette: >
  %\VignetteIndexEntry{femtograd: Introduction}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>"
)
```

Here's a quick demonstration of how to use the
[`femtograd`](https://github.com/queelius/femtograd) R package.

Load it like this:
```{r setup}
library(femtograd)
```

## Exponential distribution

Let's create a simple loglikelihood function for the exponential distribution
paramterized by $\lambda$ (failure rate).

We have
$$
  f_{T_i}(t_i | \lambda) = \lambda \exp(-\lambda t_i).
$$
So, the loglikelihood function is just
$$
  \ell(\lambda) = n \log \lambda - \lambda \sum_{i=1}^n t_i.
$$

Let's generate $n=30$ observations.
```{r}
n <- 30
true_rate <- 1
data <- rexp(n,true_rate)
head(data)

(mle.rate <- abs(1/mean(data)))
```

We see that the MLE $\hat\theta$ is `r mle.rate`.
Let's solve for the MLE iteratively as a demonstration of how to use
[`femtograd`](https://github.com/queelius/femtograd).
First, we construct the log-likelihood function.
```{r loglike_exp}
loglike_exp <- function(rate, data)
{
  return(log(rate)*length(data) - rate * sum(data))
}
```

Initially, we guess that $\hat\lambda$ is $5$, which is a terrible estimate.
```{r}
rate <- val(-100)
```

Next, we find the MLE using a simple iteration (100 loops).
```{r}
# to prevent taking too large of a step
# gradients are a local feature, so we don't want to make too big of jumps
grad_clip <- function(grad, max_norm = 1)
{
  norm <- sqrt(sum(grad * grad))
  if (norm > max_norm)
    grad <- (max_norm / norm) * grad
  grad
}

loglik <- loglike_exp(rate, data)
for (i in 1:20)
{
  zero_grad(loglik)
  backward(loglik)

  rate$data <- rate$data + 0.01 * grad_clip(rate$grad)
  cat("loglike =", loglik$data, ", rate =", rate$data, ", score =", rate$grad, "\n")
}
```

Did it converge to the MLE?
```{r}
(converged <- (abs(mle.rate - rate$data) < 1e-3))
```

It's worth pointing out that we did not update `loglik` in the loop, since
we only needed the gradient (score) to do that particular gradient ascent.
If, however, we had used the log-likelihood value at `rate`, for instance
using a line search method to avoid having to specify a step size,
then we would need to update `loglik` each time through the loop.

## Weibull distribution

Now let's do it for a slightly more complicated distribution, Weibull.

```{r}
loglike_weibull <- function(shape, scale, data)
{
  n <- length(data)
  s <- sum_values(lapply(data, function(x) (val(x) / scale)^shape))
  a <- sum_values(lapply(data, function(x) log(val(x) / scale)))
  
  val(n) * log(shape/scale) + (shape - val(1)) * a - s
}

n <- 30
true_shape <- 10
true_scale <- 29
data <- rweibull(n, true_shape, true_scale)

shape <- val(5)
scale <- val(5)
shape.v <- 0
scale.v <- 0
beta <- .9
lr <- 0.01
for (i in 1:20)
{
  loglik <- loglike_weibull(shape, scale, data)
  zero_grad(loglik)
  backward(loglik)
  
  # we updated loglik so we can show the updated log-likelihood
  cat("loglike =", loglik$data, ", dl/dk =", shape$grad, ", dl/ds =", scale$grad, "\n")

  # update velocity with momentum
  shape.v <- beta * shape.v + grad_clip(shape$grad, max_norm=10)
  scale.v <- beta * scale.v + grad_clip(scale$grad, max_norm=10)

  # update parameters
  shape$data <- shape$data + lr * shape.v
  scale$data <- scale$data + lr * scale.v
}
print(c(shape=shape$data, scale=scale$data, loglike=loglik$data))
```
