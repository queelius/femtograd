---
title: "femtograd: Introduction"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{femtograd: Introduction}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>"
)
```

Load it like this:
```{r setup}
library(femtograd)
```

Let's create a simple loglikelihood function for the exponential distribution
paramterized by $\lambda$ (failure rate).

We have
$$
  f_{T_i}(t_i | \lambda) = \lambda \exp(-\lambda t_i).
$$
So, the loglikelihood function is just
$$
  \ell(\lambda) = n \log \lambda - \lambda \sum_{i=1}^n t_i.
$$

Let's generate $n=100$ observations.
```{r}
n <- 100
rate <- val(1)
data <- rexp(n,rate$data)
score <- n / rate$data - sum(data)
head(data)

cat("loglike =", n * log(rate$data) - rate$data * sum(data), "\n")
cat("rate =", rate$data, "\n")
cat("score =", score, "\n")
```

We see that at $\hat\lambda = `r rate$data`$, the score is $`r score`$.
Let's use [`femtograd` package](https://github.com/queelius/femtograd)  to
construct the log-likelihood function.
```{r}
loglike_exp <- function(rate, data)
{
  val(length(data)) * log(rate) - rate * sum_values(data)  
}
```

Now we can use it:
```{r grad_step}
loglik <- loglike_exp(rate, data)
loglik
zero_grad(loglik)
backward(loglik)
```

What's the result?
```{r grad_step_output}
cat("loglike =", loglik$data, "\n")
cat("rate = ", rate$data, "\n")
cat("dloglik/drate =", rate$grad, "\n")
```

We see that the score is $`r rate$grad`$, which agreeds with the previous
calculation.
Now, let's set $\hat\lambda$ to $1/\bar{t}$, the MLE.
```{r}
rate <- val(1/mean(data))
loglik <- loglike_exp(rate, data)
zero_grad(loglik)
backward(loglik)
cat("loglike =", loglik$data, "\n")
cat("rate = ", rate$data, "\n")
cat("dloglik/drate =", rate$grad, "\n")
```

As expected, we see that the score is $`r rate$grad`$ (zero). Also, the
log-likelihood is larger (should be maximum) at this rate value.

Next, we show how to find the MLE using a simple iteration.
First, we set $\hat\lambda$ to $5$, a terrible estimate.
```{r}
rate <- val(5)
```

Now we loop 20 times.
```{r}
for (i in 1:20)
{
  loglik <- loglike_exp(rate, data)
  zero_grad(loglik)
  backward(loglik)
  rate <- val(rate$data + 0.01 * rate$grad)
  cat("rate =", rate$data, "\n")
}
```

Did it converge to the MLE?
```{r}
print(abs(1/mean(data) - rate$data) < 1e-3)
```
It appears so.

One thing we could do to improve the efficiency is to make the computational
graph created be lazily computed such that whenever we change a node in
the graph, whenever we retrieve the value at a node, the changes automatically
propogate.

We would have to rework the structure of `value`, though, and `femtograd`
is not intended to be a heavy-duty library for automatic differentiation.
If you need something more significant, consult another library.
