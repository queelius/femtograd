% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/optimize.R
\name{newton_raphson}
\alias{newton_raphson}
\title{Newton-Raphson optimizer}
\usage{
newton_raphson(
  objective_fn,
  params,
  max_iter = 100,
  tol = 1e-08,
  maximize = TRUE,
  step_scale = 1,
  verbose = 0
)
}
\arguments{
\item{objective_fn}{Function taking list of value parameters, returns scalar}

\item{params}{List of value objects (initial parameter values)}

\item{max_iter}{Maximum iterations, default 100}

\item{tol}{Convergence tolerance on step size, default 1e-8}

\item{maximize}{If TRUE (default), maximize; if FALSE, minimize}

\item{step_scale}{Scale factor for Newton step (< 1 for damping), default 1}

\item{verbose}{Print progress every N iterations (0 for silent)}
}
\value{
A list containing:
\item{params}{List of value objects at optimum}
\item{value}{Objective function value at optimum}
\item{gradient}{Gradient at optimum}
\item{hessian}{Hessian at optimum}
\item{iterations}{Number of iterations performed}
\item{converged}{TRUE if step size < tol}
}
\description{
Finds the optimum using Newton-Raphson method with exact Hessian.
Uses second-order information for faster convergence near optimum.
}
\details{
Newton-Raphson update: θ_{n+1} = θ_n - H⁻¹ g
For maximization, uses: θ_{n+1} = θ_n - H⁻¹ g (H is negative definite)
For minimization, uses: θ_{n+1} = θ_n - H⁻¹ g (H is positive definite)

The Hessian is computed via forward-over-reverse AD at each iteration.
This is exact but can be slow for many parameters.
}
\examples{
\dontrun{
# Find MLE for normal distribution
x <- rnorm(100, mean = 5, sd = 2)
loglik <- function(p) loglik_normal(p[[1]], p[[2]], x)
result <- newton_raphson(loglik, list(val(0), val(1)))
sapply(result$params, data)  # Should be close to c(mean(x), sd(x))
}

}
