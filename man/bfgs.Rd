% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/optimize.R
\name{bfgs}
\alias{bfgs}
\title{BFGS quasi-Newton optimizer}
\usage{
bfgs(
  objective_fn,
  params,
  max_iter = 1000,
  tol = 1e-06,
  maximize = FALSE,
  line_search_fn = NULL,
  verbose = 0
)
}
\arguments{
\item{objective_fn}{Function taking list of value parameters, returns scalar}

\item{params}{List of value objects (initial parameter values)}

\item{max_iter}{Maximum iterations, default 1000}

\item{tol}{Convergence tolerance on gradient norm, default 1e-6}

\item{maximize}{If TRUE, maximize; if FALSE (default), minimize}

\item{line_search_fn}{Line search function (default: backtracking)}

\item{verbose}{Print progress every N iterations (0 for silent)}
}
\value{
A list containing:
\item{params}{List of value objects at optimum}
\item{value}{Objective function value at optimum}
\item{gradient}{Gradient at optimum}
\item{inv_hessian}{Approximate inverse Hessian}
\item{iterations}{Number of iterations performed}
\item{converged}{TRUE if gradient norm < tol}
}
\description{
Finds the optimum using the BFGS algorithm, which approximates the
inverse Hessian using gradient information. More efficient than
Newton-Raphson when Hessian computation is expensive.
}
\details{
BFGS maintains an approximation B to the inverse Hessian, updated as:
B_{k+1} = (I - ρ\emph{s}y') B_k (I - ρ\emph{y}s') + ρ\emph{s}s'
where s = x_{k+1} - x_k, y = g_{k+1} - g_k, ρ = 1/(y'*s)

The search direction is d = -B*g, followed by line search.
}
\examples{
\dontrun{
# Minimize Rosenbrock function
rosenbrock <- function(p) {
  x <- p[[1]]; y <- p[[2]]
  (1 - x)^2 + 100 * (y - x^2)^2
}
result <- bfgs(rosenbrock, list(val(-1), val(-1)))
sapply(result$params, data)  # Should be close to c(1, 1)
}

}
