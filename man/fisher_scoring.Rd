% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/optimize.R
\name{fisher_scoring}
\alias{fisher_scoring}
\title{Fisher scoring optimizer}
\usage{
fisher_scoring(loglik_fn, params, max_iter = 100, tol = 1e-08, verbose = 0)
}
\arguments{
\item{loglik_fn}{Log-likelihood function}

\item{params}{List of value objects (initial parameter values)}

\item{max_iter}{Maximum iterations, default 100}

\item{tol}{Convergence tolerance, default 1e-8}

\item{verbose}{Print progress every N iterations (0 for silent)}
}
\value{
Same structure as newton_raphson
}
\description{
Similar to Newton-Raphson but uses expected Fisher information
(negative expected Hessian) instead of observed Hessian.
More stable for some problems.
}
\details{
For regular exponential families, Fisher scoring is equivalent to
Newton-Raphson since observed = expected information.

Fisher scoring: θ_{n+1} = θ_n + I(θ_n)⁻¹ S(θ_n)
where I = -E\link{H} (Fisher information) and S = gradient (score).

This implementation uses the observed Hessian as an approximation
to the expected Hessian, making it identical to Newton-Raphson.
For a true Fisher scoring implementation, one would need to compute
E\link{H} analytically or via Monte Carlo.
}
