% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/optimize.R
\name{gradient_ascent}
\alias{gradient_ascent}
\title{Gradient ascent/descent optimizer}
\usage{
gradient_ascent(
  objective_fn,
  params,
  lr = 0.01,
  max_iter = 1000,
  tol = 1e-06,
  maximize = TRUE,
  grad_clip = NULL,
  verbose = 0
)
}
\arguments{
\item{objective_fn}{Function taking list of value parameters, returns scalar}

\item{params}{List of value objects (initial parameter values)}

\item{lr}{Learning rate (step size), default 0.01}

\item{max_iter}{Maximum iterations, default 1000}

\item{tol}{Convergence tolerance on gradient norm, default 1e-6}

\item{maximize}{If TRUE (default), maximize; if FALSE, minimize}

\item{grad_clip}{Maximum gradient norm (NULL for no clipping)}

\item{verbose}{Print progress every N iterations (0 for silent)}
}
\value{
A list containing:
\item{params}{List of value objects at optimum}
\item{value}{Objective function value at optimum}
\item{gradient}{Gradient at optimum}
\item{iterations}{Number of iterations performed}
\item{converged}{TRUE if gradient norm < tol}
}
\description{
Finds the optimum of a function using gradient-based optimization.
Supports gradient clipping and adaptive step sizes.
}
\examples{
\dontrun{
# Find MLE for exponential distribution
x <- rexp(100, rate = 2)
loglik <- function(p) loglik_exponential(p[[1]], x)
result <- gradient_ascent(loglik, list(val(1)))
data(result$params[[1]])  # Should be close to 1/mean(x)
}

}
